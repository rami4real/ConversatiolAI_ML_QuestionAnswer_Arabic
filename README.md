# üåü Chatbot en Arabe pour le Machine Learning üåü

---

## üìã √âtapes Principales du Projet

### **1. Scraping des Donn√©es**
- **But :** Collecter des questions-r√©ponses (Q/A) pertinentes pour le machine learning en anglais.
- **Sources utilis√©es :**
  - Turing Machine Learning Questions
  - MyGreatLearning Blog
- **Outils :** Utilisation de `Beautiful Soup` (Python) pour le scraping.

---

### **2. Nettoyage de la Base de Donn√©es**
- Suppression des caract√®res sp√©ciaux, des espaces inutiles et standardisation du format des donn√©es.
- Validation manuelle pour garantir la pertinence des donn√©es collect√©es.

---

### **3. Traduction Anglais ‚Üí Arabe**
- **Outils utilis√©s :** Trois mod√®les de traduction de Hugging Face :
  - [`Helsinki-NLP/opus-mt-en-ar`](https://huggingface.co/Helsinki-NLP/opus-mt-en-ar)
  - [`marefa-nlp/marefa-mt-en-ar`](https://huggingface.co/marefa-nlp/marefa-mt-en-ar)
  - [`t5-v1_1-base`](https://huggingface.co/t5-v1_1-base)
- **R√©sultat :** Le mod√®le [`marefa-nlp/marefa-mt-en-ar`](https://huggingface.co/marefa-nlp/marefa-mt-en-ar) a produit les meilleures traductions en termes de qualit√© et de pertinence.



![marefa-nlp/marefa-mt-en-ar](marefa1.jpg)


---

### **4. Nettoyage et Formatage des Textes en Arabe**
- **Outils :** API Gemini (via prompts avanc√©s) pour :
  - Reformuler et corriger les textes.
  - G√©n√©rer des donn√©es dans le format SQuAD (Q/A structur√©), adapt√© pour l'entra√Ænement des mod√®les.
- **Approche :** Multi-shot pour garantir des exemples vari√©s et coh√©rents.

  ![marefa-nlp/marefa-mt-en-ar](gemini.jpg)


---

### **5. Fine-tuning des Mod√®les LLM**
- **Mod√®les test√©s :**
  - **Finetuned Google AI Studio** (le meilleur parmi les trois mod√®les test√©s)
  - **AraBERT**
  - **T5-Small**
- **R√©sultat :** Bien que Finetuned Google AI Studio ait surpass√© les autres mod√®les en termes de performances, aucun des trois mod√®les n'a produit des r√©sultats satisfaisants pour la t√¢che sp√©cifique.

![Performance Comparison GIF](ai_studio.gif)



---

### **6. Utilisation de Ollama et Fine-tuning**
- **Plateforme utilis√©e :** Ollama
- **Mod√®les test√©s :**
  - `Mistral 7B`
  - `Llama 3.2`
- **√âtapes :**
  - T√©l√©chargement et fine-tuning des mod√®les avec des fichiers adapt√©s (`modelfiles`).
- **Comparaison :** Le mod√®le **Mistral 7B** a d√©montr√© des performances sup√©rieures en termes de scores WSSA et de retours qualitatifs.

  
![marefa-nlp/marefa-mt-en-ar](mistral.png)


---

### **7. Interface Utilisateur**
- Cr√©ation d'une interface utilisateur avec **Ollama Open UI** pour interagir avec le chatbot.
- Investigation des mod√®les directement via l'interface pour valider leurs r√©ponses et effectuer des comparaisons.

    ![marefa-nlp/marefa-mt-en-ar](interface.png)


---
## üõ†Ô∏è Lancer les Mod√®les Mistral 7B et Llama 3.2 avec Ollama et Docker

### 1. Installation d'Ollama
Avant de commencer, installez Ollama sur votre machine en suivant ces √©tapes :

- Rendez-vous sur le site officiel d‚ÄôOllama : [https://ollama.ai](https://ollama.ai).
- T√©l√©chargez la version d‚ÄôOllama correspondant √† votre syst√®me d‚Äôexploitation (Windows, macOS ou Linux).
- Installez l‚Äôoutil en suivant les instructions sp√©cifiques √† votre plateforme.

---

### 2. T√©l√©charger les Mod√®les
Une fois Ollama install√©, utilisez les commandes suivantes pour t√©l√©charger les mod√®les n√©cessaires :

#### T√©l√©charger le mod√®le **Mistral 7B** :
```bash
ollama pull mistral7b 
```

#### T√©l√©charger le mod√®le Llama 3.2 :
```bash
ollama pull llama3.2
```
---

### 3. Cr√©er et Ex√©cuter les Mod√®les
Pour fine-tuner ou personnaliser les mod√®les avec vos propres donn√©es, utilisez la commande suivante :
```bash
ollama create -f <path-to-modelfile> <nom-du-modele>
```

- Remplacez <path-to-modelfile> par le chemin vers le fichier contenant vos donn√©es ..
- Remplacez <nom-du-modele> par le nom que vous souhaitez attribuer au mod√®le.

#### Exemple :
```bash
ollama create -f ./data/mistral_Modelfile mistral7b-custom
```
---

### 4. Lancer les Mod√®les avec Docker et Open Web UI
Si vous souhaitez interagir avec les mod√®les via une interface graphique conviviale, utilisez Open Web UI avec Docker.

#### √âtape 1 : Lancer le Conteneur Docker
Ex√©cutez la commande suivante pour lancer le conteneur Docker :
```bash

docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway \
-v open-webui:/app/backend/data --name open-webui --restart always \
ghcr.io/open-webui/open-webui:main
```

#### √âtape 2 : V√©rifier le Conteneur
Assurez-vous que Docker est install√© et actif. V√©rifiez l‚Äô√©tat du conteneur avec cette commande :
```bash
docker ps
```
#### √âtape 3 : Acc√©der √† l‚ÄôInterface
Ouvrez votre navigateur web et rendez-vous √† l'adresse suivante :

http://localhost:3000

Vous pourrez alors interagir avec les mod√®les Mistral 7B et Llama 3.2 dans une interface utilisateur intuitive.


![MON GIF](Generetive_AI_Mlaa.gif)

---
---
## √âvaluation des Mod√®les LLaMA et Mistral7B

Ce projet √©value les performances des mod√®les **LLaMA** et **Mistral7B** en utilisant diff√©rentes m√©triques pour comparer leurs r√©ponses g√©n√©r√©es dans un contexte donn√©, en particulier pour des textes en langue arabe. Nous avons commenc√© par des m√©triques classiques telles que **BLEU** et **ROUGE-L**, mais avons adopt√© une nouvelle m√©trique **WSAA** (Weighted Semantic Similarity with Arabic-specific Adjustments) pour une √©valuation plus pr√©cise.

### 1. R√©sultats **BLEU** et **ROUGE-L**

#### Table des r√©sultats

| **Question** | **Mod√®le**   | **BLEU** | **ROUGE-L** |
|--------------|--------------|----------|-------------|
| **1**        | LLaMA        | 0.0413   | 0.3000      |
|              | Mistral7B    | 0.0820   | 0.6667      |
| **2**        | LLaMA        | 0.1083   | 0.0000      |
|              | Mistral7B    | 0.0835   | 0.3478      |
| **3**        | LLaMA        | 0.0390   | 0.0000      |
|              | Mistral7B    | 0.0159   | 0.0000      |

#### Observations

Les r√©sultats des m√©triques **BLEU** et **ROUGE-L** montrent que ni **LLaMA** ni **Mistral7B** n'ont atteint des performances satisfaisantes, particuli√®rement dans la question 3 o√π les scores sont tr√®s faibles. Ces m√©triques classiques ne semblent pas adapt√©es pour cette t√¢che sp√©cifique en arabe.

---

### 2. Adopting the **WSAA** Metric

Pour une √©valuation plus pertinente, nous avons choisi d'adopter la **m√©trique WSAA** (Weighted Semantic Similarity with Arabic-specific Adjustments). Cette m√©trique prend en compte plusieurs aspects importants :

- **Coh√©rence S√©mantique** : Mesure de la similarit√© s√©mantique entre la r√©ponse g√©n√©r√©e et la r√©f√©rence.
- **Couverture de Domaine** : Mesure dans quelle mesure les termes sp√©cifiques au domaine sont couverts dans les r√©ponses g√©n√©r√©es.
- **Composants suppl√©mentaires** : BLEU et ROUGE sont √©galement pris en compte dans cette m√©trique ajust√©e pour l'arabe.

Nous avons donc calcul√© un score global en fonction de ces crit√®res.

---

### 3. R√©sultats **WSAA** (Weighted Semantic Similarity with Arabic-specific Adjustments)

#### Table des r√©sultats

| **Question** | **Mod√®le**   | **Score Final** | **Coh√©rence S√©mantique** | **Couverture de Domaine** |
|--------------|--------------|-----------------|--------------------------|---------------------------|
| **1**        | LLaMA        | 0.3163          | 0.8001                   | 0.0000                    |
|              | Mistral7B    | 0.3662          | 0.8205                   | 0.0000                    |
| **2**        | LLaMA        | 0.3225          | 0.8749                   | 0.0000                    |
|              | Mistral7B    | 0.3461          | 0.8538                   | 0.0000                    |
| **3**        | LLaMA        | 0.3425          | 0.7832                   | 0.2500                    |
|              | Mistral7B    | 0.3900          | 0.9289                   | 0.2500                    |

#### Observations

Les r√©sultats obtenus avec la m√©trique **WSAA** montrent que **Mistral7B** d√©passe **LLaMA** sur toutes les questions, avec des scores sup√©rieurs en **coh√©rence s√©mantique** et un meilleur **score final**. Mistral7B obtient √©galement un score particuli√®rement √©lev√© dans la **question 3**.

---

### 4. Conclusion : Le Meilleur Mod√®le

En conclusion, bien que **LLaMA** ait montr√© des performances initiales acceptables selon certaines m√©triques, **Mistral7B** s'av√®re √™tre le mod√®le le plus performant sur la base de notre m√©trique **WSAA**. Cela en fait le choix optimal pour g√©n√©rer des r√©ponses coh√©rentes et pertinentes, en particulier dans un contexte en arabe.

Nous recommandons donc **Mistral7B** comme mod√®le principal pour les t√¢ches de g√©n√©ration de texte dans ce domaine.

---


## üöÄ R√©sultats et Prochaines √âtapes
- **Meilleur mod√®le :** `Mistral 7B`
- **Prochaines √©tapes :**
  - Am√©liorer la robustesse du chatbot sur d'autres dialectes arabes.
  - Ajouter des m√©triques avanc√©es comme **BERTScore** pour √©valuer les r√©sultats.

---

## üìû Contact
**Auteur : [Votre Nom]**  
üìß **Email :** [votre.email@example.com]  
üåê **Portfolio :** [VotrePortfolio.com]  
üìÇ **GitHub :** [VotreGitHub](https://github.com/VotreGitHub)
